{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoqWvSTz6f7S"
   },
   "source": [
    "# Install all you need for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lo1mOEsg3_G",
    "outputId": "386fddcf-258a-48fc-e5ec-d05faf04a4ed"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jo1eyZX_4n44",
    "outputId": "1c2d6d98-1bdd-4e9a-ce9c-d7b1d8a492f8"
   },
   "outputs": [],
   "source": [
    "# Before installing those libraries, you need to install build-essential, libopenmpi-dev and pytorch with CUDA support.\n",
    "%pip install transformers\n",
    "%pip install mpi4py\n",
    "%pip install transformers[deepspeed]\n",
    "%pip install deepspeed\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngLKjg335AGy",
    "outputId": "a6c44003-3406-460a-807c-d939dfb59671"
   },
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MASTER port should be open if train with ddp\n",
    "RAnk - main gpu\n",
    "\n",
    "\"\"\"\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9994'\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"# for ddp\n",
    "os.environ['WORLD_SIZE'] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\" #uncoment for large files\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "torch.manual_seed(42)\n",
    "MODEL_DIR =  '/workspace/models/mailqa_large'\n",
    "\n",
    "device = 'cuda:0'\n",
    "backbone = 'sberbank-ai/rugpt3large_based_on_gpt2'\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(backbone, use_fast=True)\n",
    "\n",
    "train_path = '/workspace/qna.txt'\n",
    "\n",
    "def tokenize(text):\n",
    "    print(f'Tokenizing text length {len(text)}')\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "# Custom dataset loader using multiprocessing to parallelize tokenization\n",
    "class MailRuDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerFast, file_path: str, block_size: int = 1024):\n",
    "        if not os.path.exists('./cached_dataset'):\n",
    "            self.samples = []\n",
    "\n",
    "            print('Reading dataset file')\n",
    "            with open(file_path, encoding='utf-8', errors='ignore') as data_file:\n",
    "                data = data_file.read()\n",
    "\n",
    "            print(f'Data size: {len(data)}')\n",
    "\n",
    "            print('Chunking dataset file')\n",
    "            data_chunks = [data[i:i+2097152] for i in tqdm(range(0, len(data), 2097152))]\n",
    "\n",
    "            del data\n",
    "            gc.collect()\n",
    "\n",
    "            print(f'Chunk count: {len(data_chunks)}')\n",
    "\n",
    "            print('Starting tokenization')\n",
    "            with Pool(8) as p:\n",
    "                tokenized_text = [token for tokens in p.map(tokenize, data_chunks) for token in tokens]\n",
    "                p.close()\n",
    "                p.join()\n",
    "\n",
    "            del data_chunks\n",
    "            gc.collect()\n",
    "\n",
    "            print(f'Tokenized text size: {len(tokenized_text)}')\n",
    "\n",
    "            print('Splitting by block size, ignoring last sample')\n",
    "            self.samples = [tokenized_text[i:i+block_size] for i in range(0, len(tokenized_text) - block_size + 1, block_size)]\n",
    "\n",
    "            print(f'Sample count: {len(self.samples)}')\n",
    "\n",
    "            del tokenized_text\n",
    "            gc.collect()\n",
    "\n",
    "            pickle.dump(self.samples, open(\"./cached_dataset\", \"wb\"))\n",
    "\n",
    "            print('Dataset loaded and cached to disk')\n",
    "        else:\n",
    "            print('Loading cached dataset')\n",
    "            self.samples = pickle.load(open('./cached_dataset', 'rb'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.samples[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "def load_dataset(train_path, tokenizer):\n",
    "    train_dataset = MailRuDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=1024)\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "\n",
    "    return train_dataset, data_collator\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset, data_collator = load_dataset(train_path, tokenizer)\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(backbone).to(device)\n",
    "\n",
    "    training_args = TrainingArguments(output_dir=MODEL_DIR,\n",
    "                                      num_train_epochs=1, \n",
    "                                      logging_steps=50, \n",
    "                                      save_steps=2000,\n",
    "                                      per_device_train_batch_size=1,\n",
    "                                      per_device_eval_batch_size=1,\n",
    "                                      warmup_steps=100,\n",
    "                                      weight_decay=0.01, \n",
    "                                      fp16=True,\n",
    "                                      #warmup_steps=10,\n",
    "                                      #weight_decay=0.01,  \n",
    "                                      #fp16=True, \n",
    "                                      #fp16_opt_level='O1', not useful beacuse deepspeed\n",
    "                                      report_to=\"wandb\",\n",
    "                                      save_total_limit=5)\n",
    "    trainer = Trainer(model=model, args=training_args, \n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "                      \n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pB8WpDGF6TKm",
    "outputId": "5c806659-c44c-4e33-ef8b-6e197bf13d76",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 train.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Deepspeed rugpt3 large",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f45dca7dd3fddcbe10d20aa3dacb8c7f4624b30cff4229482f7d6a35bc035d63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
